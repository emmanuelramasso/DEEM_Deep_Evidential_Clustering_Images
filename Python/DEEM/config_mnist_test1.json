{
    "training_mode": "DML",
    "COMMENT_training_mode": "PRECOMPUTED_EMBEDDINGS",
    "COMMENT_EMBED": "If training mode is PRECOMPUTED_EMBEDDINGS then use it",
    "jsonfile_precompEmbeddings": "",

    "model_pretrained": "",  

    "dataset": "mnist",
    "tb_filesuffix": "test0.5priorpair",
    "model_path_DML": "../Deep_Distance_Metric_Learning/test/mnist_m8_5epochs_repo/",
    "model_name_DML": "mnist_m8_5epochs",
    "image_folder_train": "../data/mnist/train/",
    "image_folder_test": "../data/mnist/test/",
    "image_folder_valid": "../data/mnist/validation/",
    
    "COMMENT_LR": "Depending on the scheduler can be used or not see below",    
    "base_learning_rate": 0.0001,
    "batch_size": 128,
    "num_epochs": 5,
    "nitermax": 300000,
    "niterlastphase": 0,
    "min_delta_acc_earlystop": 0.0001,
    "patience_acc_earlystop": 5,
    "log_interval_test": 0.2,
    "log_interval_val": 0.2,
    
    "probprior": 0.0,
    "useDEEMprior_pair": 0,
    "useDEEMprior_label": 0,
    "prob_sampleforce_DEEM_pairs": 0.0,

    "COMMENT_prior": "if DML is not used these lines are not considered",
    "useDMLprior_pairs2labels": 0,
    "useDMLprior_pairs": 1,
    "prob_sampleforceDMLprior_pairs": 0.99,

    "dist_normalisation": "exp", 
    
    "logdir_tb": "logs_tb_mnist/",
    "numclasses": 10,
    "POUR CHANGER LA TAILLE CHANGER ICI, [3 224 224] par ex mais aussi changer dans EVNN_datasets.py -> transform 28 ou 224 manuel": 0,
    "input_shape": [1, 28, 28],
    "loss_evnn": "mse",
    "loss_prior": "mse",
    "NOT_USED_YET_weight_loss_evnn": 1.0,
    "NOT_USED_YET_weight_loss_prior": 1.0,
    "network": "resnet18",
    "COMMENTnetwork": "simple1",
    "NOT_USED_YET_thresh_mindist": 0,
    "NOT_USED_YET_thresh_maxdist": 1.0,
    "******SCHEDULER USED*******": "*************",
    "1_scheduler": "OneCycleLR",
    "2_scheduler": "StepLR",
    "3_scheduler": "CyclicLR",
    "scheduler": "ConstantLR",
    "5_scheduler": "staircase", 
    "******Param for each scheduler*******": "*************",
    "******For CyclicLR*******": "*************",
    "//COMMENT2a": "6500 maxit, Cyclic LR uses cycles. With step_size_up and step_size_down we can set the dynamics",
    "//COMMENT2a2": "Use triangular2 for amplitude decay by 1/2 each cycle, and triangular otherwise",
    "mode_CyclicLR": "triangular",
    "step_size_up_CyclicLR": 2000,
    "step_size_down_CyclicLR": 4000,
    "eta_min": 0.0001,
    "COMMENT_eta_max": "Replaced by base_learning_rate",
    "//COMMENT2b": "With nitermax = step_size_up+step_size_down+niterlastphase we have 1 cycle",
    "//COMMENT2c": "nitermax must be >= step_size_up+step_size_down+niterlastphase",
    "//COMMENT2d": "if a cyclic scheduler is used and in particuler 1 cycle the parameter niterlastphase is used for the last phase maintaining the scheduler constant",
    "*****For OneCycleLR*******": "*************",
    "COMMENT_max_learning_rate": 0.001,
    "3phases": 0,
    "*****For StepLR*******": "*************",
    "step_size_StepLR": 10,
    "gamma_StepLR": 0.90,
    "*****For ConstantLR*******": "*************",
    "gamma_constLR": 1.0,
    "nbepoch_apply_gamma": "inf",
    "*****Optimizer*******": "*************",
    "optimizer": "adam",
    "//COMMENT5": "Weight decay used if adamw used",
    "adamw_weight_decay": 0.005,
    "save_model": 0
}